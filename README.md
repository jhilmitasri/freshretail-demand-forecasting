# ğŸ›’ Stockout-Aware Product Demand Forecasting using FreshRetailNetâ€‘50K

This project aims to forecast product demand in retail stores while accounting for real-world complexities such as stockouts, promotions, holidays, and weather effects. It uses the [FreshRetailNet-50K](https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) dataset â€” a large-scale, real-world perishable goods sales dataset from 898 stores across 18 cities.

---

## ğŸŒŸ Problem Statement

> **How can we accurately forecast product demand in retail stores while accounting for stockouts, promotions, and contextual factors like weather and holidays?**

This project focuses on building a forecasting system that:

* Recovers **true latent demand** during stockout periods
* Models **temporal and contextual patterns** driving demand
* Supports inventory planning and loss prevention strategies

---

## ğŸ§  Dataset Summary

* **Source**: [FreshRetailNet-50K](https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K)
* **Granularity**: Hourly sales for 863 perishable SKUs
* **Stores**: 898
* **Time Window**: 90 days
* **Key Columns**:

  * `hours_sale`: Units sold per hour
  * `hours_stock_status`: 1 = stockout, 0 = in-stock
  * `discount`, `activity_flag`: Promotion metadata
  * `holiday_flag`, `precpt`, `avg_temperature`: Contextual features

---


## ğŸ§­ Industry Inspiration: How Major Retailers Forecast Demand

This project draws inspiration from how retail giants like Walmart, Target, and The Home Depot tackle forecasting at scale. Hereâ€™s a curated overview of their strategies:

---

ğŸ”¹ **1. AI-Powered Demand Forecasting & Stockout Prevention**
- **Walmart** uses AI to tailor inventory based on regional trends and prevent stockouts by reallocating inventory across geographies ([Walmart AI Blog](https://corporate.walmart.com/newsroom/2021/07/27/how-walmart-is-using-ai-to-make-in-store-shopping-better)).
- **Target** built a real-time â€œInventory Ledgerâ€ that uses AI to double stock coverage by factoring in real-time demand and logistics ([Target AI Ledger â€“ CNBC](https://www.cnbc.com/2021/05/18/how-target-uses-ai-to-track-and-forecast-store-inventory.html)).

---

ğŸ”¹ **2. Multi-Level Hierarchical Forecasting**
- Walmart Samâ€™s Club uses machine learning to forecast demand at the item, store, and day level, maintaining coherence across hierarchies ([Walmart Labs Forecasting](https://medium.com/walmartglobaltech/time-series-forecasting-at-scale-at-sams-club-cb13b0ce0b92)).

---

ğŸ”¹ **3. Real-Time Data Integration & Supplier Collaboration**
- **RetailLink** gives Walmart suppliers real-time access to sales and inventory levels to align restocking ([RetailLink Overview](https://retaillink.wal-mart.com)).
- Walmart pioneered **Vendor Managed Inventory (VMI)** and **Collaborative Planning, Forecasting and Replenishment (CPFR)** in the 1990s ([Walmart VMI Strategy â€“ Harvard](https://hbr.org/2004/11/the-power-of-collaboration)).

---

ğŸ”¹ **4. Weather-Aware & Event-Based Forecasting**
- Retailers like Walmart and Home Depot integrate **weather APIs** to adjust inventory for expected local surges ([IBM/The Weather Company â€“ Retail Use Case](https://www.ibm.com/blogs/industries/ai-retail-weather-forecasting/)).

---

ğŸ”¹ **5. Product & Store Segmentation for Scalability**
- Product/store clustering enables better generalization and reduces model count ([Uberâ€™s Time Series Segmentation Approach](https://eng.uber.com/ts-segmentation/)).
- Grouping by **category**, **region**, and **temporal behavior** helps scale forecasting systems.

---

ğŸ”¹ **6. Cloud-Native Pipelines for Scalability**
- Retailers use **Airflow**, **SageMaker**, and internal MLOps tools to manage training, inference, and deployment ([Walmartâ€™s MLOps Strategy](https://medium.com/walmartglobaltech/mlops-machine-learning-operations-64b4832b17f6)).

---

### âœ… Implications for This Project
- Use **granular features** (store, category, weather) in a global model.
- Apply **hierarchical forecasting** to enforce coherence across product/store levels.
- Prioritize **category-level modeling** to avoid modeling all 50k SKUs individually.
- Integrate **weather/holiday signals** to reflect contextual shifts.
- Plan for **automated retraining workflows** with tools like **Airflow** or **Prefect**.



## ğŸ“Š Exploratory Data Analysis (EDA)

### âœ”ï¸ Hourly Trends

* Peak demand between **6 AM to 9 AM**
* Stock replenishment typically occurs early in the day

### âœ”ï¸ Weekday Behavior

* Highest sales on **Tuesdays** and **Saturdays**
* Minimal activity mid-week

### âœ”ï¸ Stockout Analysis

* High stockout frequency overnight and late evening
* Low stockouts in early morning (restock hours)

### âœ”ï¸ Promotions & Discounts

* Discounts do not show clear uplift in current subset
* Needs further segmentation or larger-scale comparison

---

## ğŸ“ˆ Sample Visualizations

<p float="left">
  <img src="docs/hourly_sales.png" width="400"/>
    <img src="docs/stockout_rate.png" width="400"/>
  
</p>
<p float="left">
    <img src="docs/weekday_sales.png" width="400"/>
  <!-- <img src="docs/discount_impact.png" width="400"/> -->
</p>

---

## ğŸ§© Next Steps

* [x] **Latent Demand Recovery**: Estimate true demand during stockouts
* [x] **Daily Aggregation**: Switched from hourly to daily granularity to reduce data sparsity, improve model training stability, and accommodate memory constraints during preprocessing.
* [ ] **Train forecasting models** (LightGBM, LSTM, TFT)
* [ ] **Evaluate performance on multiple time horizons**
* [ ] **Integrate store-level and category-level predictions**
* [ ] **Visualize final model predictions across store-product groups**

---

## ğŸ“‚ Project Structure

```
.
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda.ipynb
â”‚   â”œâ”€â”€ category_store_analysis.ipynb
â”‚   â”œâ”€â”€ latent_demand_forecasting.ipynb
â”‚   â”œâ”€â”€ product_level_demand_imputation.ipynb
â”‚   â””â”€â”€ 05_daily_baseline_modeling.ipynb   â† ğŸ†• NEW
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ daily_dataset
â”‚   â”œâ”€â”€ freshretail_flattened_chunks/   # Full hourly data split into parquet chunks
â”‚   â”œâ”€â”€ flattened_chunks/   
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ hourly_sales.png
â”‚   â”œâ”€â”€ stockout_rate.png
â”‚   â””â”€â”€ weekday_sales.png
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ“Œ Dependencies

```bash
pip install -r requirements.txt
```

---


## âš ï¸ Modeling Decisions

* Initially attempted modeling at hourly level (24 values/day) but encountered instability due to memory issues and extreme sparsity during late hours.
* Switched to **daily aggregation** to simplify the pipeline and ensure better generalization across categories and stores.
* Flagged full-day stockout days and anomalous sales during stockouts instead of dropping them â€” preserving integrity of training samples.
* Initially tested aggregation over **6 AM to 10 PM**, but reverted to **full-day (24 hours)** to maintain consistency and reduce leakage from selective hour exclusions.

_ğŸ“ Note: These trade-offs are logged for future benchmarking and ablation studies._

## ğŸ™Œ Credits

* Dataset by [Dingdong-Inc](https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K)
* Inspired by operational research in retail demand planning


## Notebooks Overview

ğŸ”¹ 01_eda.ipynb

Objective: Perform foundational exploratory data analysis on the full FreshRetailNet dataset.
Key Steps:
- Identified unique products (865), stores (898), and ~50,000 product-store combinations.
- Plotted category and store distributions to assess modeling feasibility.
- Helped guide whether to model per-product/store or via aggregation.

ğŸ”¹ 02_category_store_analysis.ipynb

Objective: Analyze demand distribution across categories and stores to design scalable modeling groups.
Key Steps:
- Identified top 20 third-level categories globally and top 5 per store.
- Found 87 unique categories cover 83.1% of demand.
- Mapped each of these top categories to associated store IDs for selective model training.

ğŸ”¹ 03_latent_demand_forecasting.ipynb

Objective: Set up demand forecasting pipelines focused on the 87 high-impact third-level categories.
Key Steps:
- Filtered dataset to only include relevant categories.
- Calculated total demand coverage.
- Prepared modeling granularity plan to balance performance with scalability.

ğŸ”¹ 04_product_level_demand_imputation.ipynb

Objective: Impute missing or latent demand signals at the product level prior to forecasting.
Key Steps:
- Designed a strategy to estimate imputed demand using hours-level stock/sales signals.
- Merged and aligned imputed values with the master dataset.
- Enabled cleaner downstream modeling by reducing signal sparsity.
  
ğŸ”¹ 05_daily_baseline_modeling.ipynb

Objective: Aggregate hourly sales data into daily format and prepare a baseline dataset for modeling.
Key Steps:
- Aggregated hourly features (sales, stockouts, weather, promotions) to daily granularity.
- Flagged full-day stockout periods (`oos_hours_24 == 24`) and suspicious sales during those periods.
- Switched from 6â€“22 hour filtering to full 24-hour retention due to inconsistencies in industrial reporting.
- Finalized `daily_df` for top third-level categories covering 90â€“95% of total sales volume.


ğŸ”¹ 06_model_training_analysis.ipynb

Objective: Analyze baseline model performance and refine training approach.
Key Steps:
- Evaluated LightGBM dailyâ€baseline RMSE/MAE per category.
- Reviewed feature importance and correlation diagnostics (ACF/PACF).
- Logged model stability issues and prepared for recursive/direct strategies.

ğŸ”¹ 07_imputation_and_aggregation.ipynb

Objective: Implement scalable imputation and aggregation pipeline for hourlyâ†’daily transformation.
Key Steps:
- Chunked read of flattened hourly parquet files.
- Raw and inâ€stock group aggregations with vectorized imputation.
- Exported final daily dataset with imputed sales.

ğŸ”¹ 08_feature_engineering.ipynb

Objective: Build comprehensive time series features for modeling.
Key Steps:
- Generated multiple lags (1,7,14 days) and rolling statistics.
- Added calendar encodings (day_of_week, weekend, time_idx).
- Integrated contextual features (stockouts, weather, promotions).

ğŸ”¹ 08_model_recursive.ipynb

Objective: Train recursive autoregressive LightGBM models per category using skforecast.
Key Steps:
- Prepared perâ€category train/validation splits.
- Fitted `ForecasterRecursive` with lag features and exogenous variables.
- Reported perâ€category RMSE and compared against baseline.

ğŸ”¹ 09_direct_sliding_window.ipynb

Objective: Establish direct multiâ€step slidingâ€window forecasting baseline.
Key Steps:
- Constructed fixedâ€length lag windows as features.
- Trained multiâ€output regressors for 7â€day forecasts.
- Benchmarked against recursive and baseline models.

ğŸ”¹ 10_sequence_modeling.ipynb

Objective: Prototype sequenceâ€toâ€sequence forecasting in pure PyTorch.
Key Steps:
- Implemented encoderâ€decoder architectures.
- Built custom `DataLoader` and training loops.
- Assessed initial performance and GPU feasibility.

ğŸ”¹ 11_Sequence_Modelling_GPU.ipynb

Objective: Accelerate sequence modeling with PyTorch Forecasting (TFT).
Key Steps:
- Converted daily data into `TimeSeriesDataSet`.
- Trained Temporal Fusion Transformer on MPS/CUDA.
- Logged missingâ€timestep handling, model convergence, and attention plots.

ğŸ”¹ 12-darts-n-beats.ipynb

Objective: Experiment with Darts Nâ€‘BEATS forecasting at category level.
Key Steps:
- Aggregated daily category series and filled missing dates.
- Configured Nâ€‘BEATSModel with cyclic encoders.
- Ran backtests and plotted predicted vs actual sales.

ğŸ”¹ 14-darts-n-beats.ipynb

Objective: Follow-up on Nâ€‘BEATS hyperparameter and encoder studies.
Key Steps:
- Tuned block/types, stack_depth, and encoder settings.
- Evaluated additional temporal encodings (datetime attributes).
- Compared performance against baseline regressors.