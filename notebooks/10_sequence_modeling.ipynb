{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea14811",
   "metadata": {},
   "source": [
    "# 10: Sequence Modeling with Temporal Fusion Transformer\n",
    "\n",
    "**Goal:** Build a TFT on our daily imputed demand to learn seasonality, lags, and exogenous effects automatically‚Äîno manual lag‚Äêengineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871404c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhilmitasri/Repositories/MyRepositories/freshretail-demand-forecasting/env/lib/python3.12/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n",
      "/Users/jhilmitasri/Repositories/MyRepositories/freshretail-demand-forecasting/env/lib/python3.12/site-packages/pytorch_forecasting/models/base/_base_model.py:28: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31cd3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_PATH = \"data/daily_dataset/daily_df_imputed.parquet\"\n",
    "df = pd.read_parquet(DAILY_PATH)\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"])\n",
    "df[\"third_category_id\"]   = df[\"third_category_id\"].astype(str)\n",
    "df[\"store_id\"]            = df[\"store_id\"].astype(str)\n",
    "df[\"management_group_id\"] = df[\"management_group_id\"].astype(str)\n",
    "df[\"time_idx\"] = (df[\"dt\"] - df[\"dt\"].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffed8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length    = 28\n",
    "max_prediction_length = 7\n",
    "training_cutoff       = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "tft_dataset = TimeSeriesDataSet(\n",
    "    df[df[\"time_idx\"] <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"daily_sale_imputed\",\n",
    "    group_ids=[\"third_category_id\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"third_category_id\"],\n",
    "    time_varying_known_reals=[\"time_idx\",\"discount\",\"oos_hours_total\",\"holiday_flag\"],\n",
    "    time_varying_unknown_reals=[\"daily_sale_imputed\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"third_category_id\"], transformation=\"softplus\"),\n",
    "    allow_missing_timesteps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f48140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = tft_dataset.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_dataloader = tft_dataset.to_dataloader(\n",
    "    train=False, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2fac2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 3.7 K  | train\n",
      "3  | prescalers                         | ModuleDict                      | 80     | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 48     | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 3.0 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.4 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676    | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 17     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "22.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.9 K    Total params\n",
      "0.092     Total estimated model params size (MB)\n",
      "282       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhilmitasri/Repositories/MyRepositories/freshretail-demand-forecasting/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/19140 [00:00<?, ?it/s] "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "\n",
    "# callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "lr_logger  = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "# model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    tft_dataset,\n",
    "    learning_rate=3e-3,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1,               # <‚Äî single value per timestep\n",
    "    loss=RMSE(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=3\n",
    ")\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    callbacks=[early_stop, lr_logger],\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# fit\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82a682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
